---
title: "Myopic Study_Contd"
author: "Wasi Naqvi 90501180"
date: "2023-11-05"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown



$\textbf{Section 1: Bootstrapping the Myopic Study Data}$

```{r}

library(readr)

library(dplyr) 
df <- read.csv("myopia.csv", sep = ";", header = TRUE)
train <- sample(1:n,n*0.6)
test <- setdiff(1:n,train)
train_data <- df[train, ]
test_data <- df[test, ]
testing_data<-test_data

logreg1 <- glm(MYOPIC ~ SPHEQ + AL + ACD, data = train_data, family = binomial)


beta.est <- function(data, index) {
 
  data_subset <- data[index, ]
  
  logreg<- glm(factor(MYOPIC) ~ SPHEQ + AL + ACD, data = data_subset, family = binomial)
  
  coef_estimates <- coef(logreg)
  
  
  coefficients <- coef_estimates[c("SPHEQ", "AL", "ACD")]
  
  return(coefficients)
}

set.seed(90501180+58843)
library(boot)
boot_results<-boot(df,beta.est,1000)



print(boot_results)
par(mfrow = c(1, 3))  # Set up a 1x3 grid for histograms

hist(boot_results$t[, 1], breaks = 30, col = "skyblue", 
     xlab = "Coefficient Estimate", ylab = "Frequency", 
     main = "Bootstrap Estimates for SPHEQ ")
hist(boot_results$t[, 2], breaks = 30, col = "lightgreen", 
     xlab = "Coefficient Estimate", ylab = "Frequency", 
     main = "Bootstrap Estimates for AL")
hist(boot_results$t[, 3], breaks = 30, col = "salmon", 
     xlab = "Coefficient Estimate", ylab = "Frequency", 
     main = "Bootstrap Estimates for ACD")


legend("topright", legend = c("SPHEQ", "AL", "ACD"), 
       fill = c("skyblue", "lightgreen", "salmon"), title = "Coefficients")


bootstrap_std_errors <- boot_results$se

glm_std_errors <- summary(logreg1)$coefficients[, "Std. Error"]
# Displaying the results
# Calculate standard deviations across columns (coefficients)
bootstrap_sd <- apply(boot_results$t, 2, sd)
# Display the standard deviations
cat("Standard Deviations of Bootstrap Estimates:\n")
cat("SPHEQ:", bootstrap_sd[1], "\n")
cat("AL:", bootstrap_sd[2], "\n")
cat("ACD:", bootstrap_sd[3], "\n")
cat("Standard Errors from glm() function:\n")
cat("SPHEQ:", glm_std_errors["SPHEQ"], "\n")
cat("AL:", glm_std_errors["AL"], "\n")
cat("ACD:", glm_std_errors["ACD"], "\n")
#Question 5
cat("The Standard Errors are pretty similar. However, the log reg error given by the glm() function is a little bit higher for each coefficient.")
```




The Standard Errors are pretty similar. However, the log reg error given by the glm() function is a little bit higher for each coefficient






$\textbf{Section 2:Salaray Analysis}$

```{r}

salaries <- read.csv("datasalaries.csv", header = TRUE)
str(salaries)
salaries$company=as.factor(salaries$company)
salaries$gender=as.factor(salaries$gender)
salaries$Education=as.factor(salaries$Education)
salaries$Race=as.factor(salaries$Race)


set.seed(21321)
library(rsample)
dat_split <- initial_split(salaries, prop = 0.75)
train_data <- training(dat_split)
test_data <- testing(dat_split)

library(tree)



tree_model <- tree(totalyearlycompensation ~ ., data = salaries)

summary(tree_model)


options(repr.plot.width = 10, repr.plot.height = 8)  # Adjust plot size
plot(tree_model)
text(tree_model, pretty = 0)

cat("Apple and Google tend to have the higher salaries!!")


set.seed(90501180) # set seed to make work reproducible
tree.cv <- cv.tree(tree_model)
plot(tree.cv, type="b")
cat("4 terminal nodes is suggested as shown in the plot!!")


pruned_tree <- prune.tree(tree_model, best = 4)


plot(pruned_tree)
text(pruned_tree,pretty=0)
cat("The Predicted Salary for the Hispanic Male with a Bachelors,is $403100")


tree.pred <- predict(tree_model, data = train_data)
mse_pruned<-mean((tree.pred-test_data$totalyearlycompensation)^2)




set.seed(53453)
library(randomForest)

rf_model <- randomForest(totalyearlycompensation ~ ., data = train_data, ntree = 500, importance = TRUE)

# Print the model
print(rf_model)



predicted_test <- predict(rf_model, newdata = test_data)


test_mse1 <- mean((test_data$totalyearlycompensation - predicted_test)^2)
print(paste("Test MSE:", test_mse))


oob_mse1 <- mean((rf_model$y - rf_model$predicted)^2)
print(paste("OOB MSE:", oob_mse))

#Question 15
varImpPlot(rf_model)
cat("Years of Experience is most important for rfmodel")

#Question 13
rf_model2 <- randomForest(totalyearlycompensation ~ ., data = train_data, ntree = 500, importance = TRUE,mtry=2)

#Question 14
predicted_test2 <- predict(rf_model2, newdata = test_data)
test_mse2 <- mean((test_data$totalyearlycompensation - predicted_test2)^2)
print(paste("Test MSE:", test_mse))


oob_mse2 <- mean((rf_model2$y - rf_model2$predicted)^2)
print(paste("OOB MSE:", oob_mse))

# Print the model
print(rf_model2)


#Question 15
varImpPlot(rf_model2)


cat("Years of Experience is most important for rfmodel2")


```


Apple and Google tend to have the higher salaries!!

$\textbf{Section 3: LDA and QDA}$


```{r}


#Question 16
library(gbm)

# Set seed for reproducibility
set.seed(47484)


boost_model <- gbm(
  formula = totalyearlycompensation ~ .,
  distribution = "gaussian",
  data = train_data,
  n.trees = 5000,
  interaction.depth = 4
)

# Print the model summary
summary(boost_model)

#Question 17
# Predictions on the test set
predicted_test <- predict(boost_model, newdata = test_data, n.trees = 5000)

# Calculate MSE on the test set
test_mse_boost <- mean((test_data$totalyearlycompensation - predicted_test)^2)
test_mse_boost

```





```{r}

#Question 18
# Create a table to compare the MSEs
mse_table <- data.frame(
  Model = c("Pruned Tree", "Model 1", "Model 2", "Boosted Model", "OOB Model 1", "OOB Model 2"),
  Test_MSE = c(mse_pruned, test_mse1, test_mse2, test_mse_boost, NA, NA),
  OOB_MSE = c(NA, NA, NA, NA, oob_mse1, oob_mse2)
)

# Print the MSE comparison table
mse_table


```
Question 18: 
As we can see by the test mse given by each model.

The best is 
Model 2 (Bagging) with the mtry parameter=2
Then 
Model 1 (Bagging)
Then
Boosted Model
And the Worst Model is The Pruned Tree!!!

Question 19:

Yes It agrees! The OOB for Model 2 is the lowest!!

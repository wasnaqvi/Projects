{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa7448a2",
   "metadata": {},
   "source": [
    "# Ariel spectroscopy ranking — combined plots + eclipse score saturation diagnostics\n",
    "\n",
    "This notebook loads your exported ranked lists:\n",
    "\n",
    "- `tpc_transit_ranked.csv`\n",
    "- `tpc_eclipse_ranked.csv`\n",
    "\n",
    "and reproduces / extends the combined visualizations **with Transit vs Eclipse overlaid**, then diagnoses why the **eclipse scores saturate near 1** and offers two fixes:\n",
    "\n",
    "1) **Post-hoc tie-breaker scoring** (no retraining; fastest)\n",
    "2) **Cleaner retrain**: exclude availability from features + use a **count likelihood** (Negative Binomial) so the model can produce a wider range of required events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85746cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- paths ----------\n",
    "transit_csv = Path(\"tpc_transit_ranked.csv\")\n",
    "eclipse_csv = Path(\"tpc_eclipse_ranked.csv\")\n",
    "\n",
    "assert transit_csv.exists(), f\"Missing {transit_csv.resolve()}\"\n",
    "assert eclipse_csv.exists(), f\"Missing {eclipse_csv.resolve()}\"\n",
    "\n",
    "tr = pd.read_csv(transit_csv)\n",
    "ec = pd.read_csv(eclipse_csv)\n",
    "\n",
    "# IMPORTANT: guard against hidden whitespace in headers\n",
    "tr.columns = tr.columns.str.strip()\n",
    "ec.columns = ec.columns.str.strip()\n",
    "\n",
    "tr.head(), ec.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- column names used throughout ----------\n",
    "TR_SCORE = \"transit_spectroscopy_score\"\n",
    "EC_SCORE = \"eclipse_spectroscopy_score\"\n",
    "\n",
    "TR_PRED  = \"pred_Tier1Transits_med\"\n",
    "EC_PRED  = \"pred_Tier1Eclipses_med\"\n",
    "\n",
    "TR_PFEAS = \"p_pred_le_available_transits\"\n",
    "EC_PFEAS = \"p_pred_le_available_eclipses\"\n",
    "\n",
    "TR_AVAIL = \"Available Transits\"\n",
    "EC_AVAIL = \"Available Eclipses\"\n",
    "\n",
    "needed = [\n",
    "    (tr, [TR_SCORE, TR_PRED, TR_PFEAS, TR_AVAIL]),\n",
    "    (ec, [EC_SCORE, EC_PRED, EC_PFEAS, EC_AVAIL]),\n",
    "]\n",
    "for df, cols in needed:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(\"Missing columns:\", missing)\n",
    "        print(\"Available score-like columns:\", [c for c in df.columns if \"score\" in c.lower()])\n",
    "        raise KeyError(missing)\n",
    "\n",
    "# ensure sorted best→worst\n",
    "tr = tr.sort_values(TR_SCORE, ascending=False).reset_index(drop=True)\n",
    "ec = ec.sort_values(EC_SCORE, ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Transit N =\", len(tr), \"Eclipse N =\", len(ec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3eef35",
   "metadata": {},
   "source": [
    "## 1) Quick saturation check (why eclipse scores look “overfit”)\n",
    "\n",
    "If your score is basically:\n",
    "\n",
    "\\[\n",
    "\\text{score} \\approx \\frac{\\Pr(\\text{required} \\le \\text{available})}{\\text{required (median)}}\n",
    "\\]\n",
    "\n",
    "then it will **saturate near 1** when:\n",
    "\n",
    "- predicted required events are often **≈ 1**, and\n",
    "- feasibility \\(\\Pr(\\cdot)\\) is often **≈ 1** (because available events are large)\n",
    "\n",
    "That’s not “overfitting” in the usual sense — it’s **metric saturation / degeneracy**: many rows get essentially the same best-possible score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_float(df, col):\n",
    "    return pd.to_numeric(df[col], errors=\"coerce\").to_numpy(float)\n",
    "\n",
    "def summarize(df, score_col, pred_col, pfeas_col, avail_col):\n",
    "    s = _as_float(df, score_col)\n",
    "    pred = _as_float(df, pred_col)\n",
    "    pfeas = _as_float(df, pfeas_col)\n",
    "    avail = _as_float(df, avail_col)\n",
    "    return {\n",
    "        \"N\": len(df),\n",
    "        \"score_min\": np.nanmin(s),\n",
    "        \"score_med\": np.nanmedian(s),\n",
    "        \"score_max\": np.nanmax(s),\n",
    "        \"frac_score_eq_1\": np.nanmean(np.isclose(s, 1.0)),\n",
    "        \"pred_med_min\": np.nanmin(pred),\n",
    "        \"pred_med_med\": np.nanmedian(pred),\n",
    "        \"pred_med_max\": np.nanmax(pred),\n",
    "        \"frac_pred_med_eq_1\": np.nanmean(np.isclose(pred, 1.0)),\n",
    "        \"pfeas_med\": np.nanmedian(pfeas),\n",
    "        \"frac_pfeas_eq_1\": np.nanmean(np.isclose(pfeas, 1.0)),\n",
    "        \"avail_med\": np.nanmedian(avail),\n",
    "        \"avail_min\": np.nanmin(avail),\n",
    "        \"avail_max\": np.nanmax(avail),\n",
    "    }\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Transit\": summarize(tr, TR_SCORE, TR_PRED, TR_PFEAS, TR_AVAIL),\n",
    "    \"Eclipse\": summarize(ec, EC_SCORE, EC_PRED, EC_PFEAS, EC_AVAIL),\n",
    "}).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts: do we only predict a few discrete required-event levels?\n",
    "print(\"Transit predicted required (median) value counts:\")\n",
    "display(pd.to_numeric(tr[TR_PRED], errors=\"coerce\").value_counts().sort_index())\n",
    "\n",
    "print(\"\\nEclipse predicted required (median) value counts:\")\n",
    "display(pd.to_numeric(ec[EC_PRED], errors=\"coerce\").value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- diagnostic plots: required / feasibility / availability ----------\n",
    "C_TR = \"tab:blue\"\n",
    "C_EC = \"tab:orange\"\n",
    "\n",
    "tr_pred  = _as_float(tr, TR_PRED)\n",
    "ec_pred  = _as_float(ec, EC_PRED)\n",
    "tr_pfeas = _as_float(tr, TR_PFEAS)\n",
    "ec_pfeas = _as_float(ec, EC_PFEAS)\n",
    "tr_av   = _as_float(tr, TR_AVAIL)\n",
    "ec_av   = _as_float(ec, EC_AVAIL)\n",
    "\n",
    "plt.figure(figsize=(7.5, 4.8))\n",
    "plt.hist(tr_pred[np.isfinite(tr_pred)], bins=np.arange(0.5, np.nanmax(tr_pred)+1.5, 1), alpha=0.6, label=\"Transit\", color=C_TR)\n",
    "plt.hist(ec_pred[np.isfinite(ec_pred)], bins=np.arange(0.5, np.nanmax(ec_pred)+1.5, 1), alpha=0.6, label=\"Eclipse\", color=C_EC)\n",
    "plt.xlabel(\"Predicted required Tier-1 events (median)\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Predicted required events distribution\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7.5, 4.8))\n",
    "plt.hist(tr_pfeas[np.isfinite(tr_pfeas)], bins=30, alpha=0.6, label=\"Transit\", color=C_TR)\n",
    "plt.hist(ec_pfeas[np.isfinite(ec_pfeas)], bins=30, alpha=0.6, label=\"Eclipse\", color=C_EC)\n",
    "plt.xlabel(\"P(required ≤ available)\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Feasibility distribution\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7.5, 4.8))\n",
    "plt.hist(np.log10(tr_av[np.isfinite(tr_av)]), bins=40, alpha=0.6, label=\"Transit\", color=C_TR)\n",
    "plt.hist(np.log10(ec_av[np.isfinite(ec_av)]), bins=40, alpha=0.6, label=\"Eclipse\", color=C_EC)\n",
    "plt.xlabel(\"log10(Available events)\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Availability distribution (log10 scale)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9e8b4",
   "metadata": {},
   "source": [
    "## 2) Reproduce the combined plots you showed (overlay Transit + Eclipse)\n",
    "\n",
    "These match your combined figures:\n",
    "- score drop-off vs rank\n",
    "- feasibility vs required (Pareto-ish view)\n",
    "- depth vs brightness (marker size ∝ score)\n",
    "- score distributions (overlaid histogram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transit_depth_frac(df):\n",
    "    d = pd.to_numeric(df.get(\"Transit Depth [%]\"), errors=\"coerce\").to_numpy(float)\n",
    "    return d / 100.0\n",
    "\n",
    "def tess_mag(df):\n",
    "    return pd.to_numeric(df.get(\"Star TESS Mag\"), errors=\"coerce\").to_numpy(float)\n",
    "\n",
    "def size_from_score(s, smin=None, smax=None):\n",
    "    s = np.asarray(s, float)\n",
    "    if smin is None: smin = np.nanmin(s)\n",
    "    if smax is None: smax = np.nanmax(s)\n",
    "    return 20 + 240 * (s - smin) / (smax - smin + 1e-12)\n",
    "\n",
    "tr_score = _as_float(tr, TR_SCORE)\n",
    "ec_score = _as_float(ec, EC_SCORE)\n",
    "\n",
    "# ---------- Score drop-off ----------\n",
    "plt.figure(figsize=(7.5, 4.8))\n",
    "plt.plot(np.arange(1, len(tr)+1), tr_score, label=\"Transit\", color=C_TR)\n",
    "plt.plot(np.arange(1, len(ec)+1), ec_score, label=\"Eclipse\", color=C_EC)\n",
    "plt.xlabel(\"Rank (1 = best)\")\n",
    "plt.ylabel(\"spectroscopy score\")\n",
    "plt.title(\"Score drop-off across ranked lists (Transit vs Eclipse)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- Feasibility vs required ----------\n",
    "topN = 80\n",
    "plt.figure(figsize=(7.2, 6.2))\n",
    "plt.scatter(tr_pred, tr_pfeas, alpha=0.18, label=\"Transit\", color=C_TR)\n",
    "plt.scatter(ec_pred, ec_pfeas, alpha=0.18, label=\"Eclipse\", color=C_EC)\n",
    "plt.scatter(tr_pred[:topN], tr_pfeas[:topN], alpha=0.95, color=C_TR)\n",
    "plt.scatter(ec_pred[:topN], ec_pfeas[:topN], alpha=0.95, color=C_EC)\n",
    "plt.xlabel(\"Predicted required Tier-1 events (median)\")\n",
    "plt.ylabel(\"P(required ≤ available)\")\n",
    "plt.title(f\"Feasibility vs required events (top {topN} highlighted)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- Depth vs brightness (marker size ∝ score) ----------\n",
    "tr_depth = transit_depth_frac(tr)\n",
    "ec_depth = transit_depth_frac(ec)\n",
    "tr_mag = tess_mag(tr)\n",
    "ec_mag = tess_mag(ec)\n",
    "\n",
    "all_scores = np.concatenate([tr_score[np.isfinite(tr_score)], ec_score[np.isfinite(ec_score)]])\n",
    "smin, smax = np.nanmin(all_scores), np.nanmax(all_scores)\n",
    "tr_ms = size_from_score(tr_score, smin=smin, smax=smax)\n",
    "ec_ms = size_from_score(ec_score, smin=smin, smax=smax)\n",
    "\n",
    "plt.figure(figsize=(7.8, 5.6))\n",
    "plt.scatter(tr_depth, tr_mag, s=tr_ms, alpha=0.20, label=\"Transit\", color=C_TR)\n",
    "plt.scatter(ec_depth, ec_mag, s=ec_ms, alpha=0.20, label=\"Eclipse\", color=C_EC)\n",
    "plt.xscale(\"log\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Transit Depth (fraction)\")\n",
    "plt.ylabel(\"Star TESS Mag (brighter = higher)\")\n",
    "plt.title(\"Depth vs brightness (marker size ∝ score)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- Score distributions ----------\n",
    "plt.figure(figsize=(7.5, 4.8))\n",
    "plt.hist(tr_score[np.isfinite(tr_score)], bins=40, alpha=0.55, label=\"Transit\", color=C_TR)\n",
    "plt.hist(ec_score[np.isfinite(ec_score)], bins=40, alpha=0.55, label=\"Eclipse\", color=C_EC)\n",
    "plt.xlabel(\"spectroscopy score\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Score distributions (Transit vs Eclipse)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a30a2c",
   "metadata": {},
   "source": [
    "## 3) Fix #1 (fast): post-hoc **tie-breaker** score for eclipse\n",
    "\n",
    "When many eclipse rows have **score ≈ 1**, your list is dominated by ties.  \n",
    "A simple fix (without retraining) is:\n",
    "\n",
    "- keep the feasibility/required score as the **base**\n",
    "- break ties using **brightness** (IR/TESS mags), **planet temperature** (for eclipse), and **depth SNR** (for transit)\n",
    "\n",
    "This keeps your intended meaning (“feasibility / required”) while still ranking targets sensibly *within the saturated region*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_rank(x):\n",
    "    x = np.asarray(x, float)\n",
    "    out = np.zeros_like(x, float)\n",
    "    ok = np.isfinite(x)\n",
    "    if ok.sum() < 2:\n",
    "        return out\n",
    "    r = x[ok].argsort().argsort().astype(float)\n",
    "    out[ok] = r / (len(r) - 1)\n",
    "    return out\n",
    "\n",
    "def mag_to_flux(m):\n",
    "    m = np.asarray(m, float)\n",
    "    out = np.full_like(m, np.nan, float)\n",
    "    ok = np.isfinite(m)\n",
    "    out[ok] = 10.0 ** (-0.4 * m[ok])\n",
    "    return out\n",
    "\n",
    "def ir_flux_proxy(df):\n",
    "    # prefer IR mags if present; otherwise fallback to TESS\n",
    "    mags = []\n",
    "    for c in [\"Star J Mag\", \"Star H Mag\", \"Star Ks Mag\", \"Star W1 Mag\", \"Star W2 Mag\"]:\n",
    "        if c in df.columns:\n",
    "            mags.append(mag_to_flux(pd.to_numeric(df[c], errors=\"coerce\").to_numpy(float)))\n",
    "    if len(mags) > 0:\n",
    "        return np.nanmean(np.vstack(mags), axis=0)\n",
    "    return mag_to_flux(tess_mag(df))\n",
    "\n",
    "def depth_snr(df):\n",
    "    depth = pd.to_numeric(df.get(\"Transit Depth [%]\"), errors=\"coerce\").to_numpy(float)\n",
    "    dlo = pd.to_numeric(df.get(\"Transit Depth Error Lower [%]\"), errors=\"coerce\").to_numpy(float)\n",
    "    dhi = pd.to_numeric(df.get(\"Transit Depth Error Upper [%]\"), errors=\"coerce\").to_numpy(float)\n",
    "    sig = 0.5 * (np.abs(dlo) + np.abs(dhi))\n",
    "    return (depth / np.maximum(sig, 1e-12))\n",
    "\n",
    "# --- build tie-breaker terms ---\n",
    "tr_base = tr_score.copy()\n",
    "ec_base = ec_score.copy()\n",
    "\n",
    "tr_bright = percentile_rank(ir_flux_proxy(tr))   # brighter -> higher\n",
    "ec_bright = percentile_rank(ir_flux_proxy(ec))\n",
    "\n",
    "tr_depthsnr = percentile_rank(depth_snr(tr))\n",
    "ec_teq = pd.to_numeric(ec.get(\"Planet Temperature [K]\"), errors=\"coerce\").to_numpy(float)\n",
    "ec_teq_rank = percentile_rank(ec_teq)            # hotter -> higher (for eclipse)\n",
    "\n",
    "# refined scores:\n",
    "# Transit: break ties with brightness + depth SNR\n",
    "tr_refined = tr_base * (0.6 + 0.2*tr_bright + 0.2*tr_depthsnr)\n",
    "\n",
    "# Eclipse: break ties with brightness + Teq\n",
    "ec_refined = ec_base * (0.6 + 0.2*ec_bright + 0.2*ec_teq_rank)\n",
    "\n",
    "# show whether eclipse saturation is reduced\n",
    "plt.figure(figsize=(7.5, 4.8))\n",
    "plt.hist(ec_base[np.isfinite(ec_base)], bins=40, alpha=0.55, label=\"Eclipse base\", color=\"tab:orange\")\n",
    "plt.hist(ec_refined[np.isfinite(ec_refined)], bins=40, alpha=0.55, label=\"Eclipse refined\", color=\"tab:red\")\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Eclipse score saturation: base vs refined (tie-breaker)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# write new ranked files\n",
    "tr_out = tr.copy()\n",
    "ec_out = ec.copy()\n",
    "tr_out[\"transit_score_refined\"] = tr_refined\n",
    "ec_out[\"eclipse_score_refined\"] = ec_refined\n",
    "\n",
    "tr_out = tr_out.sort_values(\"transit_score_refined\", ascending=False).reset_index(drop=True)\n",
    "ec_out = ec_out.sort_values(\"eclipse_score_refined\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "tr_out.to_csv(\"tpc_transit_ranked_refined.csv\", index=False)\n",
    "ec_out.to_csv(\"tpc_eclipse_ranked_refined.csv\", index=False)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\" - tpc_transit_ranked_refined.csv\")\n",
    "print(\" - tpc_eclipse_ranked_refined.csv\")\n",
    "\n",
    "# quick peek\n",
    "cols_show_tr = [\"Planet Name\",\"Star Name\",TR_SCORE,\"transit_score_refined\",TR_PRED,TR_PFEAS,TR_AVAIL,\"Star TESS Mag\",\"Planet Temperature [K]\"]\n",
    "cols_show_ec = [\"Planet Name\",\"Star Name\",EC_SCORE,\"eclipse_score_refined\",EC_PRED,EC_PFEAS,EC_AVAIL,\"Star TESS Mag\",\"Planet Temperature [K]\"]\n",
    "cols_show_tr = [c for c in cols_show_tr if c in tr_out.columns]\n",
    "cols_show_ec = [c for c in cols_show_ec if c in ec_out.columns]\n",
    "\n",
    "display(tr_out[cols_show_tr].head(15))\n",
    "display(ec_out[cols_show_ec].head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b45011e",
   "metadata": {},
   "source": [
    "## 4) Fix #2 (cleaner): remove **availability** from the model features (avoid leakage)\n",
    "\n",
    "If your feature set includes `Available Transits` / `Available Eclipses`, the model can partially learn schedule/availability structure,\n",
    "which then makes predictions collapse toward **“required ≈ 1”** whenever availability is large.\n",
    "\n",
    "Cleaner setup:\n",
    "\n",
    "- **Model predicts required events** from: depth + depth errors + duration errors + stellar magnitudes + Teq  \n",
    "  (exclude `Available ...`)\n",
    "- **Scoring uses availability only in feasibility**: \\(\\Pr(\\text{required} \\le \\text{available})\\)\n",
    "\n",
    "Below is a drop-in sketch of a better likelihood for counts, too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda2c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- COUNT likelihood idea (recommended for targets like 'Tier 1 Eclipses') ----\n",
    "# Instead of modeling log(count) as a continuous variable, model counts directly with a log link.\n",
    "# This produces integer predictive draws and typically a wider range of required-event predictions.\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "def negbin_count_model(x, y=None):\n",
    "    \"\"\"Negative Binomial regression: y ~ NB(mean=exp(b0 + x beta), dispersion=phi).\"\"\"\n",
    "    n, p = x.shape\n",
    "    b0 = numpyro.sample(\"b0\", dist.Normal(0, 1))\n",
    "    beta = numpyro.sample(\"beta\", dist.Normal(0, 1).expand((p,)))\n",
    "    # dispersion / overdispersion (larger phi -> closer to Poisson)\n",
    "    phi = numpyro.sample(\"phi\", dist.HalfNormal(1.0))\n",
    "\n",
    "    eta = b0 + jnp.dot(x, beta)\n",
    "    mu = jnp.exp(eta)\n",
    "\n",
    "    # NB parameterization in NumPyro: NegativeBinomial2(mean=mu, concentration=phi)\n",
    "    numpyro.sample(\"y\", dist.NegativeBinomial2(mean=mu, concentration=phi), obs=y)\n",
    "\n",
    "# Notes:\n",
    "# - You can keep your existing StandardScaler + imputer.\n",
    "# - SVI works well for this too.\n",
    "# - Scoring: use posterior predictive draws y_draws (counts) directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898979c",
   "metadata": {},
   "source": [
    "### Minimal scoring change if you switch to a count likelihood\n",
    "\n",
    "If `y_draws` are integer required-event draws:\n",
    "\n",
    "- `pred_required_med = median(y_draws)`\n",
    "- `p_feasible = mean(y_draws <= available)`\n",
    "- `score = p_feasible / pred_required_med`\n",
    "\n",
    "This usually avoids the “everything ≈ 1” collapse if the model can predict a broader count range.\n",
    "\n",
    "---\n",
    "\n",
    "### If you want, I can convert your current `ariel_ranking_no_waic_with_mags_v2.py` into a notebook\n",
    "and implement the Negative Binomial option behind a CLI flag like:\n",
    "\n",
    "- `--likelihood lognormal` (current)\n",
    "- `--likelihood negbin` (recommended for counts)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
